NOTES:
For running:
    - We should be in the 'src' folder to run the. Do not run 'main.py' from any other folder.

For evaluation: Caffe model should be present in the corresponding directory in the evaluation package
    - Change dir to src/evaluation/third_party/caffemodel and, check the bash file and run it


OBSERVATIONS:
1. A very interesting observation I had during training the model on MNIST was that when I trained the model using the
raw values provided in the dataset (gray-scale images where each pixel has a value from 0-255), the model had difficulty
to produce any meaningful digit, all it was producing was simply meaningless noise.
However, when I normalized the 8-bit value of each pixel to the 0-1 interval (by dividing by 255) it was able to produce
digits very well. Interestingly, it seems that the huge variation between the pixel values in the case of the raw values
disabled the model to learn the patterns, while the normalized values help the model better understand and generate the
patterns.

2. num_workers=4 is problematic on T4 as well


Installations:
I used the following command to install Caffe in a new environment. By default installing it showed me errors becuase of conflicts
in cudatoolkit version required by this and PyTprch. The version of Scipy==1.0.0 is needed to Cityscpaes evaluation (it has
the imsave function which is now deprecated).
=====> conda create -n caffe scipy=1.0.0 numpy Pillow caffe-gpu

Note: The Caffe model could only be used when activating the caffe workshop on the clusters. This environment is only
useful for this task as it has almost nothing else installed.

Evaluation should only be done in the "caffe" environment. The caffe package (CPU version) in the workshop environment
shall not be used for evaluation (it is too slow and has not been tested).


To-Do: make the installations all in a yaml file.
I installed comet via "pip install comet-ml", since installing via conda caused errors.
For some reason, I had to install torchvision separately after installing PyTorch: conda install -c pytorch torchvision
Also need to install mlxtnd: conda install -c conda-forge mlxtend



Important notes for evaluation:
    - The 'evaluate' script can only work if images are saved and resize with scipy=1.0.0.
    - Before evaluating, the images need to be resized to 256x256 (see the 'prepare' scripts), and they will automatically
      be resized to match the size of the actual segmentation/label inside the script.
    - The semantic classifier was trained on 256x256 images that were up-sampled to 1024x2048.
    - The script needed to download the caffe model could be found in the 'examples.sh' file.
    - Examples of resizing the generated images before evaluating could be found in the 'examples.sh' file.


Steps for getting FCN score for a model:
    - Determine the desired model via arguments
    - Run program to train the model
    - Run program for inference on validation set
    - Run program for resizing for fcn
    - Go to the evaluation package and follow the scripts for computing the scores

# IMPORTANT ASSUMPTION: the program is run from the main.py module and all the paths are considered to be from there.

Resized images and evaluation results are stored in /Midgard rather than /local_storage to make transferring the results
easier




NOTE on choosing the boundary map:
In the current implementation, the boundary maps are down-sampled to spatially match the dimension of the z in the
corresponding Block (before squeezing). Then the are squeezed in the Blocks (like z's) before being concatenated in the
affine coupling layer. In each block, the boundary maps would have a channel dimension of 12 all the time, since the
3-channel image is squeezed so it would have 12 channels.



LATEST CHANGES:
Code changes: no use of --nearest_neighbor and set do_ceil to False in CityDataset and glow.py
Helper compute_path()


"""
Notes:
    - Image names are always based on real images, even when their corresponding condition is needed.
    - Every new argument should be checked with in compute_paths function
"""


Number of params for 128x256 images:
    - baseline: 175.5M
    - act_norm conditional: 176.2M
    - w_conditional: 177.2M
    - act_norm_conditional + w_conditional: 177.9M



Note: SSIM computation needs no GPU (inference on validation images should already be done)
All the sbatch or sh files should be run from the outside of the glow2 folder, they will cd to the corresponding
folder they need for running.




========= For code maintainance:
For training, functions that need to change for a new dataset or direction:
    - compute_paths in helper package
    - do_forward, take_samples, arrange_rev_cond in interface.py in model package
    - prep_coupling_conds in two_glows
    - prepare_reverse_cond, init_model in utility.py in models.package  ==> they should be moved to interface
    - train function in train.py and calc_val_loss and take_sample
    - block.py and two_glows.py and flow.py